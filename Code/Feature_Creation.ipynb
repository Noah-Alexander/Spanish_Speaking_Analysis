{
 "cells": [
  {
   "cell_type": "raw",
   "id": "07400fe2-0c70-42c4-9c4f-4b48253ca780",
   "metadata": {},
   "source": [
    "The purpose of this file is to explore and create different features that will be used in the analysis. Specifically, measurements of interest from the transcripts. The product of this file will be a dataframe saved as a .csv file with the desired features, which will be joined to the manually collected data (by the day it represents of course) and will be analyzed / explored in another file. Below are the desired features\n",
    "\n",
    "1. Total Words Said\n",
    "2. Total \"Filler Words\" Said\n",
    "3. Unique Words Said\n",
    "4. Average Word Use Frequency\n",
    "5. Median Word Use Frequency\n",
    "6. Entropy of Word Frequency\n",
    "7. Lexical Diversity (Type-Token Ratio, or TTR), TTR = Unique Words Said / Total Words Said\n",
    "\n",
    "All except for #2 are straight forward in terms of calculation. Special note- TTR is the inverse of the Average, I'm going to have both and keep this in mind when sharing results\n",
    "\n",
    "The code outline is as follows:\n",
    "\n",
    "1. Definition of Functions\n",
    "    a. Video Number -> Data Dictionary of Video\n",
    "    b. Functions Which Return Desired Features\n",
    "2. Function Calls and Saving of Features into .csv file\n",
    "3. Exploration of Trascripts to Calculate Feature #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820bf914-4ffe-4ad0-bdaa-ed293fcac670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Global Variables\n",
    "p_tc_path = Path('./../Data/Processed_Transcripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba7c230d-dde1-4b55-b4e6-aa33b5fdb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(transcript_number):\n",
    "    \"\"\"Takes transcript number and returns a data dictionary for the corresponding transcript\"\"\"\n",
    "    name = ''\n",
    "    if transcript_number/10 < 1:\n",
    "        name = 'p_tc_' + '00' + str(transcript_number) + '.json'\n",
    "    elif transcript_number/100 < 1:\n",
    "        name = 'p_tc_' + '0' + str(transcript_number) + '.json'\n",
    "    else:\n",
    "        name = 'p_tc_' + str(transcript_number) + '.json'\n",
    "    file_name = p_tc_path / name\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file) #ensure_ascii=False ensures us that accents and ~ are maintained\n",
    "    return data\n",
    "\n",
    "def calc_total(data):\n",
    "    \"\"\"Calculates the total number of words said in the video\"\"\"\n",
    "    return sum(data.values())\n",
    "\n",
    "def calc_filler(data):\n",
    "    \"\"\"Calculates the number of 'Filler' words said in the video\"\"\"\n",
    "    num = 0\n",
    "    for w, n in data.items():\n",
    "        if w in ['ah', 'eh', 'hm', 'mm', 'm', 'aa']:\n",
    "            num += n\n",
    "    return num\n",
    "\n",
    "def calc_unique(data):\n",
    "    \"\"\"Calculates the number of unique words said in the video\"\"\"\n",
    "    return len(data.keys())\n",
    "\n",
    "def calc_average(data):\n",
    "    \"\"\"Calculates the average number of times a word was used in the video\"\"\"\n",
    "    return sum(data.values()) / len(data.values())\n",
    "\n",
    "def calc_median(data):\n",
    "    \"\"\"Calculates the median number of times a word was used in the video\"\"\"\n",
    "    return statistics.median(data.values())\n",
    "\n",
    "def calc_entropy(data):\n",
    "    \"\"\"Calculates the entropy of the vocabulary used in the video\"\"\"\n",
    "    word_counts = np.array(list(data.values())) #makes it easier to do the probabilities calculation\n",
    "    probabilities = word_counts / word_counts.sum()\n",
    "    return entropy(probabilities, base=2)\n",
    "\n",
    "def calc_ttr(data):\n",
    "    \"\"\"Calculates the TTR (unique words / total words) for the data\"\"\"\n",
    "    return len(data.values()) / sum(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117a147d-9dbe-47cf-be23-eab604217089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Video_Number  Total_Words  Filler_Words  Unique_Words  Avg_Word_Freq  \\\n",
      "0             1         1077           118           296       3.638514   \n",
      "1             2          790            76           273       2.893773   \n",
      "2             3          527            48           209       2.521531   \n",
      "3             4          933           111           306       3.049020   \n",
      "4             5          615            56           214       2.873832   \n",
      "\n",
      "   Med_word_Freq   Entropy       TTR  \n",
      "0            1.0  7.017286  0.274838  \n",
      "1            1.0  6.966307  0.345570  \n",
      "2            1.0  6.710372  0.396584  \n",
      "3            1.0  6.954690  0.327974  \n",
      "4            1.0  6.628696  0.347967  \n"
     ]
    }
   ],
   "source": [
    "#Load every associated json file, calculate measurements, save to pandas dataframe, and export to csv\n",
    "\n",
    "# List to store all results\n",
    "data_list = []\n",
    "\n",
    "\n",
    "for video_number in range(112): \n",
    "    word_dict = get_data(video_number + 1)  # Load word frequency dictionary\n",
    "\n",
    "    #Calculate results and append as a dictionary\n",
    "    data_list.append({\n",
    "        \"Video_Number\": video_number+1,\n",
    "        \"Total_Words\": calc_total(word_dict),\n",
    "        \"Filler_Words\": calc_filler(word_dict),\n",
    "        \"Unique_Words\": calc_unique(word_dict),\n",
    "        \"Avg_Word_Freq\": calc_average(word_dict),\n",
    "        \"Med_word_Freq\": calc_median(word_dict),\n",
    "        \"Entropy\": calc_entropy(word_dict),\n",
    "        \"TTR\": calc_ttr(word_dict)\n",
    "    })\n",
    "\n",
    "#Convert to a dataframe\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "#Show first 5 rows to make sure things look fine\n",
    "print(df.head())\n",
    "\n",
    "#Save to csv file\n",
    "df.to_csv(\"./../data/transcript_features.csv\", index=False)\n",
    "\n",
    "#Transcript Analysis done! Now we just have features, and we need to join and clean to the hand gathered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5a7b281-2c4a-4807-b323-5592c256ffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077\n",
      "118\n",
      "296\n",
      "3.6385135135135136\n",
      "1.0\n",
      "7.017285734407039\n",
      "0.2748375116063138\n"
     ]
    }
   ],
   "source": [
    "print(calc_total(t))\n",
    "print(calc_filler(t))\n",
    "print(calc_unique(t))\n",
    "print(calc_average(t))\n",
    "print(calc_median(t))\n",
    "print(calc_entropy(t))\n",
    "print(calc_trr(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7127c3e8-a461-46e7-a7fc-ed40c642b68f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'voy', 'tuve', 'spanish', 'terminé', 'noah', 'galicia', 'viviendo', 'qué', 'carapate', 'dedicarme', 'palabras', 'tenía', 'pasando', 'necesita', 'mirando', 'has', 'leyendo', 'viendo', 'recordando', 'covid', 'c', 'hm', 'semanas', 'matemáticas', 'youtube', 'meses', 'estudiando', 'sé', 'trabajadores', 'todos', 'vídeo', 'podcast', 'aprendiendo', 'tiene', 'estamos', 'nios', 'prácticas', 'estaba', 'mm', 'idiomas', 'todas', 'terminado', 'loo', 'libros', 'cco', 'quiero', 'usando', 'restaurantes', 'fue', 'trabajando', 'adquiriendo', 'chao', 'vídeos', 'horas', 'iguales', 'empez', 'hace', 'eres', 'dónde', 'minutos', 'estudios', 'dreaming', 'frases', 'puede', 'informática', 'emocionado', 'sabía', 'llegué', 'tr', 'quiere', 'quisiera', 'cómo', 'haciendo', 'ar', 'periodo', 'contando', 'cosas', 'días', 'problemas', 'siento', 'quería', 'empezando', 'video', 'categorías', 'intercambiar', 'puedo', 'axiliar', 'tiendas', 'entiendo', 'use', 'gradué', 'primeras', 'muchas', 'm', 'entendiendo', 'necesito', 'historias', 'unos', 'esty', 'hablando', 'traté', 'estoy', 'empecé', 'pude', 'está', 'preguntándome', 'personas', 'podré', 'sabro'}\n",
      "{'nada', 'octubre', 'vos', 'cerebro', 'el', 'para', 'diferente', 'uso', 'ha', 'saber', 'eso', 'inglés', 'como', 'que', 'cámara', 'así', 'pasar', 'en', 'tengo', 'terminar', 'si', 'media', 'mis', 'e', 'medir', 'pasado', 'yo', 'nativa', 'traducir', 'estudiar', 'útil', 'caro', 'nativo', 'ella', 'reconocido', 'tiempo', 'es', 'poro', 'poco', 'partido', 'serie', 'persona', 'real', 'usar', 'nombre', 'bien', 'cinco', 'he', 'primer', 'aquí', 'nivel', 'ahora', 'pues', 'fútbol', 'o', 'eh', 'muy', 'tener', 'trabajo', 'entender', 'llegar', 'un', 'la', 'pero', 'mi', 'hoy', 'año', 'no', 'mejor', 'durante', 'esos', 'explicar', 'porque', 'y', 'hacer', 'orgulloso', 'esposa', 'a', 'otro', 'por', 'escuchar', 'ver', 'vocabulario', 'conversación', 'español', 'tú', 'cuando', 'hola', 'camino', 'velocidad', 'sumando', 'posición', 'menos', 'larga', 'hasta', 'grande', 'explicación', 'también', 'progreso', 'siguiente', 'antes', 'solamente', 'mucho', 'total', 'casi', 'segundo', 'primavera', 'me', 'empezar', 'gramática', 'dos', 'esta', 'mayo', 'verano', 'anoche', 'estado', 'algo', 'japonés', 'rápido', 'día', 'sí', 'mes', 'mañana', 'esperar', 'tres', 'de', 'las', 'trato', 'son', 'este', 'del', 'sobre', 'vida', 'junio', 'tema', 'libro', 'contar', 'mayor', 'con', 'una', 'todo', 'proceso', 'cada', 'aprender', 'recordar', 'bueno', 'suficiente', 'mapa', 'lo', 'enseñar', 'ah', 'encontrar', 'música', 'españa', 'agua', 'hablar', 'universidad', 'los', 'quizás', 'alguien', 'listo', 'más', 'idioma'}\n",
      "Non-Spanish Words: 109/296\n",
      "Error Rate: 36.82%\n"
     ]
    }
   ],
   "source": [
    "#I am going to explore some different packages to see if a package can give me some easy calculations that look correct with a few different tests\n",
    "\n",
    "# pyspellchecker checks whether or not a word was spelled correctly, and it has functionality in Spanish\n",
    "# Idea- if a word is spelled incorrectly, it is mispronounced or not a word! \n",
    "#Let's see if it works\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(language='es')  # Spanish spell checker\n",
    "\n",
    "def count_non_words(word_list):\n",
    "    misspelled = spell.unknown(word_list)\n",
    "    print(misspelled)\n",
    "    print(spell.known(word_list))\n",
    "    return len(misspelled)\n",
    "\n",
    "transcribed_words = list(t.keys())\n",
    "non_word_count = count_non_words(transcribed_words)\n",
    "\n",
    "print(f\"Non-Spanish Words: {non_word_count}/{len(transcribed_words)}\")\n",
    "print(f\"Error Rate: {non_word_count / len(transcribed_words) * 100:.2f}%\")\n",
    "\n",
    "#The first list is words that are misspelled. It includes many words that are correctly spelled\n",
    "#The second list is words that are spelled correctly. It has \"ah\", which I'm pretty sure is not a word\n",
    "\n",
    "#This package is not functional for my purposes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "078d43e9-3154-48dd-a169-377a0048fd50",
   "metadata": {},
   "source": [
    "Not seen in this file are my attempts to use spacy and nltk, bulky nlp libraries. I had problems getting either one to just be imported into this notebook due to their sheer size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "421ffba4-afea-4171-9ae1-057e3179ae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'noah', 'esta', 'estoy', 'haciendo', 'eso', 'está', 'terminado', 'cosas', 'hace', 'hm', '5', 'meses', 'aprendiendo', 'quería', 'empezando', 'a', 'semanas', 'empez', 'estaba', 'periodo', 'necesito', 'nios', 'palabras', 'quiero', 'mis', 'frases', 'hablando', 'dónde', 'tengo', 'mm', '600', 'horas', 'categorías', 'carapate', 'empecé', 'estudiando', 'unos', 'tiene', 'prácticas', 'problemas', 'historias', 'loo', '224', '28', '2024', '7', 'estamos', 'viviendo', 'galicia', 'ella', 'axiliar', 'trabajando', 'gradué', 'tuve', 'terminé', 'sabía', 'fue', 'covid', 'podré', 'dedicarme', 'estudios', 'matemáticas', 'todos', 'todas', 'iguales', 'esty', 'usando', 'dreaming', 'spanish', 'mirando', 'viendo', 'vídeos', '92', '300', '17', 'podcast', '36', 'leyendo', 'libros', '14', 'llegué', 'pude', 'trabajadores', 'tiendas', 'restaurantes', 'preguntándome', 'traté', 'personas', 'idiomas', 'quisiera', 'necesita', 'adquiriendo', 'eres', 'has', 'siento', '55', '160', 'primeras', '120', 'contando', 'entiendo', 'entendiendo', 'pasando', 'emocionado', 'quiere', 'puedo', 'cco', 'días', 'tr', 'youtube', '95', 'puede', '11', 'minutos', 'voy', '100', '4', '6', '8', 'nativa', 'muchas', '5000', '6000', 'sé', 'cómo', '1000', '2000', 'recordando', 'tenía', 'sabro', 'use']\n",
      "['hola', 'mi', 'nombre', 'y', 'este', 'primer', 'video', 'de', 'serie', 'sobre', 'qué', 'ah', 'la', 'tema', 'del', 'hoy', 'por', 'eh', 'que', 'esperar', 'cuando', 'así', 'pues', 'como', 'más', 'o', 'menos', 'yo', 'español', 'hablar', 'dos', 'mucho', 'porque', 'antes', 'en', 'un', 'camino', 'aprender', 'idioma', 'no', 'mejor', 'posición', 'tiempo', 'para', 'ar', 'cerebro', 'los', 'traducir', 'con', 'alguien', 'bueno', 'pero', 'ahora', 'casi', 'empezar', 'm', 'libro', 'gramática', 'poco', 'vocabulario', 'el', 'hacer', 'aquí', 'sí', 'mayo', 'octubre', 'esposa', 'españa', 'hasta', 'junio', 'trabajo', 'enseñar', 'inglés', 'me', 'universidad', 'mayor', 'año', 'estudiar', 'una', 'otro', 'primavera', 'japonés', 'llegar', 'segundo', 'persona', 'vida', 'real', 'tener', 'informática', 'las', 'son', 'mapa', 'saber', 'entender', 'durante', 'verano', 'también', 'solamente', 'si', 'trato', 'encontrar', 'intercambiar', 'conversación', 'larga', 'nivel', 'bien', 'suficiente', 'grande', 'todo', 'proceso', 'tú', 'útil', 'listo', 'reconocido', 'explicación', 'lo', 'sumando', 'media', 'nativo', 'total', 'pasar', 'escuchar', 'música', 'contar', 'e', 'mes', 'muy', 'ver', 'anoche', 'partido', 'fútbol', 'quizás', 'he', 'estado', 'poro', 'velocidad', 'rápido', 'algo', 'orgulloso', 'cinco', 'c', 'pasado', 'nada', 'agua', 'cámara', 'vos', 'medir', 'progreso', 'cada', 'día', 'siguiente', 'tres', 'usar', 'caro', 'esos', 'recordar', 'diferente', 'ha', 'uso', 'terminar', 'vídeo', 'mañana', 'explicar', 'chao']\n"
     ]
    }
   ],
   "source": [
    "#I have a dictionary txt file for spanish. Let's see what happens when I use that as my \"truth\" for if a word is a word\n",
    "\n",
    "with open(\"spanish_words.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    spanish_words = {word.strip() for word in file}\n",
    "\n",
    "words = []\n",
    "not_words = []\n",
    "\n",
    "for word in t.keys():\n",
    "    if word in spanish_words:\n",
    "        words.append(word)\n",
    "    else:\n",
    "        not_words.append(word)\n",
    "\n",
    "print(not_words)\n",
    "print(words)\n",
    "\n",
    "#This doesn't work either, estoy is a word but it isn't showing conjugations or plural adjectives sadly :(\n",
    "#Looks like we will do a manual examination of what words are filler words and use those for our measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6350b743-acf5-4a2f-b03f-89a66f2a85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101856\n",
      "7969\n",
      "6064\n",
      "16.796833773087073\n",
      "1.0\n",
      "8.153054312296534\n",
      "0.05953502984605718\n",
      "['mi', 'es', 'y', 'de', 'ah', 'la', 'eh', 'hm', '5', 'o', 'yo', 'a', 'en', 'un', 'no', 'ar', 'mm', 'm', 'el', 'sí', '28', '7', 'me', '92', '17', '36', '14', 'si', 'tú', 'lo', '55', 'e', 'he', 'tr', '95', 'c', '11', '4', '6', '8', 'sé', 'ha', '29', 'sp', '90', '2', 'mí', 'se', 'in', '10', 'le', 'él', '20', 'ir', '70', '30', 'h', 'ía', 'ac', '1', 'su', 'b', 'v', 'al', '3', 'va', '31', 'pu', 'ho', 'ti', 'a3', 'h3', '16', 'd', 'f', 'g', 's', '15', 'é', '60', 'os', '25', 'oy', 'vi', '13', 'tu', 'bp', 'g3', 'n', 'l', '40', '50', '24', 'te', '80', 'so', '9', 'nu', '74', 'u', 'ya', '44', '45', 'io', 'bz', '18', 've', '12', 'oí', 'i', 'di', 'gg', 'có', 'uf', 'oj', 'ej', '2s', '19', 'c2', 'f3', '21', 'bo', 'oo', 'pé', '22', 'oh', '23', '33', 'uh', 'gr', '26', 'má', 'pr', '27', 'mo', 'yi', 't', 'qu', 'da', 'hd', 'ay', 'fr', 'on', 'aí', 'st', 'ó', '$', 'lr', 'í', 'x', 'xx', 'ni', 'cl', 'do', 'p', '35', 'r', '34', '42', 'tn', 'k', '$1', 'ch', '32', 'cu', 'ev', '4%', 'yy', 'fe', 'to', 'bi', 'ob', 're', 'ms', 'er', 'té', 'sa', 'as', 'aa', 'am', 'an', 'ca', 'vo', 'xv', 'mu', '9%', 'bu', 'z', 'ot', '0%', 'cv', 'ao', 'yó', 'dí', '99', '43', 'po', 'c1', 'añ', '88', '87', '$2', 'li', '66', 'yc', 'yí', 'ro', 'ma', '98', 'ab', 'du', 'w']\n",
      "Number of Filler Words: 7969\n"
     ]
    }
   ],
   "source": [
    "#Idea- search from smallest to largest words. Identify the buffer words (which are between 1-2 letters)\n",
    "#I'm going to join the every dictionary, sort the words, and also do some overall measurements just to have\n",
    "\n",
    "#Join all video dictionaries\n",
    "super_dict = {}\n",
    "\n",
    "for i in range(112):\n",
    "    d = get_data(i+1)\n",
    "    for w, n in d.items():\n",
    "        if w in super_dict:\n",
    "            super_dict[w] += n\n",
    "        else:\n",
    "            super_dict[w] = n\n",
    "\n",
    "#Basic statistics of the whole spoken dataset\n",
    "print(calc_total(super_dict))\n",
    "print(calc_filler(super_dict))\n",
    "print(calc_unique(super_dict))\n",
    "print(calc_average(super_dict))\n",
    "print(calc_median(super_dict))\n",
    "print(calc_entropy(super_dict))\n",
    "print(calc_trr(super_dict))\n",
    "\n",
    "#only print keys with less than 3 characters\n",
    "word_list = []\n",
    "for w, n in super_dict.items():\n",
    "    if len(w) < 3:\n",
    "        word_list.append(w)\n",
    "\n",
    "print(word_list)\n",
    "\n",
    "#List of filler words I'll count: ah, eh, hm, mm, m, aa\n",
    "\n",
    "#Note: There are words that don't make sense here, like oj, fr, ch, etc. I've noticed that sometimes a word like \"nos\" is written as \"no s\". \n",
    "#Some of these letters can be from situations like that, mispronunciations, me saying letters, etc.\n",
    "#There is a pretty big asterisk in general that comes from the fact that the transcripts are auto generated, my accent isn't what it was prepared for,\n",
    "#   and the recording situation was not the highest quality (not talking too loud many times because it is late and people in the house are asleep)\n",
    "#With high powered NLP, this could be resolved after a lot of work, research, and thought. But that isn't the purpose of this project! The purpose\n",
    "#   is to showcase my analysis abilities, not my NLP skills\n",
    "\n",
    "#How many words are filler?\n",
    "num = 0\n",
    "for w, n in super_dict.items():\n",
    "    if w in ['ah', 'eh', 'hm', 'mm', 'm', 'aa']:\n",
    "        num += n\n",
    "\n",
    "print(\"Number of Filler Words:\", num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
